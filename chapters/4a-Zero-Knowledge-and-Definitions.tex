% !TEX root = ..\Main.tex
\chapter{Formal Definitions}
\label{chapterlabel:Definitions}

\section{Notation and Computational Model}

This thesis is concerned with algorithms modelled as probabilistic polynomial-time Turing machines, and interactive probabilistic polynomial-time Turing machines. We use the abbreviations PPT and DPT for algorithms running in probabilistic polynomial time and deterministic polynomial time respectively, where the running time is polynomial in the size of the algorithm inputs. Unless otherwise stated, the sizes of inputs and outputs to the machines will be bounded above by a polynomial in a security parameter $\sep$, usually provided to the algorithms in unary form as $1^\sep$.

For functions $f,g: \N \rightarrow [0,1] $, we write $f(\sep)\approx g(\sep) $ if 
$|f(\sep) - g(\sep)|= \frac{1}{\sep^{\omega(1)}}$. We say a function $f$ is \emph{overwhelming} if 
$f(\sep) \approx 1$ and $f$ is \emph{negligible} if $f(\sep) \approx 0$.

Write $y=A(x;r)$ when the algorithm $A$ outputs $y$ on input $x$ with randomness $r$. We write $y \gets A(x)$ to mean selecting $r$ at random and setting $y=A(x;r)$. We write $y \gets S$ for sampling $y$ uniformly at random from a set $S$. We define $[n]$ to be the set of integers $1,\ldots,n$.

We use $\F$ to denote a finite field. We use bold letters such as $\vec{v}$ for row vectors. For $\vec{v}\in \F^n$ and a set $J=\{j_1,\dots, j_k\}\subset [n]$ with $j_1<\dots<j_k$ we define the vector $\vec{v}|_J$ to be $(\vec{v}_{j_1},\dots, \vec{v}_{j_k})$. Similarly, for a matrix $V\in \F^{m\times n}$ we let $V|_{J}\in \F^{m\times k}$ be the submatrix of $V$ restricted to the columns indicated in $J$.

Let $z_1,\ldots,z_m$ be distinct points in a finite field $\F$. and let $l_1(X),\ldots,l_m(X)$ be their associated Lagrange polynomials. Explicitly, $$l_i(X) = \prod_{j \neq i} \frac{X-z_j}{z_i - z_j}$$ Note that $l_i(z_j)=\delta_{i,j}$, so it is easy to see how to combine the Lagrange polynomials to produce a polynomial which interpolates a given function at $z_1, \ldots, z_m$. Let $l_0(X) = \prod_{i=1}^m (X-z_i)$. 


\section{Arithmetic Circuits}\label{sec:AC}

Arithmetic circuits are a model for algebraic computation over fields. An arithmetic circuit consists of addition and multiplication gates. Our satisfiability arguments consider arithmetic circuits described as a list of multiplication gates together with a set of linear consistency equations relating the inputs and outputs of the gates. 
%
In this section, we show how to reduce an arbitrary arithmetic circuit to this format.

\begin{definition} An arithmetic circuit over a field $\F$ and variables $(A_1,\ldots,A_m)$ is a directed acyclic graph whose vertices are called gates. Gates of in-degree 0 are inputs to the circuit and labelled with some $A_i$ or a constant field element. All other gates are labelled $+$ or $\times$.
\end{definition}

Given field elements $a_i \in \F$, an arithmetic circuit is evaluated in several steps. First, label the inputs with the $a_i$. Then, take gates whose inputs are all labelled with field elements, apply the operation on the gate to the inputs and write the answer on the output wire. and repeating this process until all gates have been labelled with output field elements.

We may consider fan-in 2 circuits, in which case all of the $+$ and $\times$ gates have in-degree 2, or arbitrary fan-in circuits. We consider circuits with arbitrary fan-out, in which case all of the $+$ and $\times$ gates have unbounded out-degree.

Arithmetic circuits can be measured in various ways. The size of an arithmetic circuit is the number of gates in the circuit. This can be further split into the number of addition gates and the number of multiplication gates. The depth of a circuit is the length of the longest path beginning from any circuit input. It is easy to see that arithmetic circuits compute polynomial functions of their inputs, and the degree of the arithmetic circuit is the total degree of the polynomial that it computes.

Arithmetic circuits can be described alternatively as a list of multiplication gates with a collection of linear consistency equations relating the inputs and outputs of the gates. Our zero-knowledge protocols for circuit satisfiability use circuits in this form. Any circuit described as an acyclic graph can be efficiently converted into the alternative description.

At a high level, we transform an arithmetic circuit into two kinds of equations. 
Multiplication gates are directly represented as equations of the form $a\cdot b=c$, where $a,b,c$ represent the left, right and output wires. We will arrange these values in matrix form producing a Hadamard matrix product. This process will lead to duplicate values, when a wire is the output of one multiplication gate and the input of another, or when it is used as input multiple times. We keep track of this by using a series of linear constraints. For example, suppose we have two multiplication gates with wire values $a_1,b_1,c_1$ and $a_2,b_2,c_2$. If the output of the first multiplication gate is the right input of the second, we would write $c_1 - b_2 = 0$.

We also add linear constraints representing the addition and multiplication by constant gates of the circuit. We then rewrite those equations so that the only wires that are referenced in the equations are those linked to (non-constant) multiplication gates. %This is always possible if we allow for some preprocessing of the initial circuit. 
We now describe this process.

\subsection{Preprocessing Arithmetic Circuits for Arguments}\label{sec:preprocAC}

We show how to remove addition and multiplication-by-constant gates  from an arithmetic circuit $A$, and replace them with bilinear consistency equations on the inputs and outputs of the remaining gates, such that satisfiability of the equations is equivalent to satisfiability in the original circuit.

 Let $B$ be the sub-circuit of $A$ containing all wires and gates before a multiplication gate, with $m$ input wires and $n$ output wires. Label the $m$ inputs of $B$ with the unit vectors $\vec{e}_i=(0,\ldots,1,\ldots,0)$ of length $m$. For every addition gate with inputs labelled as $\vec{x},\vec{y}$, label the output wire as $\vec{x}+\vec{y}$. For every multiplication-by-constant gate with inputs $\vec{x}$ and constant $c$ label the output with $c\vec{x}$. By proceeding inductively, the $n$ outputs of $B$ are now labelled with vectors of length $m$ representing them as linear combinations of the inputs.

This requires at most $m\left| B \right|$ arithmetic operations. Note however that all outputs of $B$ are linear combinations of the inputs, and that $B$ can be written with $n(2m-1)$ fan-in 2 gates in such a way that the consistency equations can be trivially read off from the circuit description. More specifically, a linear combination $\sum_{i=1}^m a_i x_i$ can be produced using $m$ multiplication-by-constant gates and $m-1$ addition gates to add the answers together.

We can now remove the gates of $B$ from $A$. We also remove any multiplication gates whose inputs are the inputs of the new circuit. Now we simply repeat the process of finding consistency equations until we have considered the whole of~$A$. In Figure~\ref{fig:digraph} there is an example of a circuit together and the corresponding consistency equations.



The first (input) and final (output) sub-circuits require additional processing. We show how to do this for the output sub-circuit. The input sub-circuit is very similarly handled.

Let $B$ be the output sub-circuit. Write $(a_1,\ldots,a_m)=\vec{a}$ for the input wires of $B$ and $(b_1,\ldots,b_n)=\vec{b}$ for the output wires. Without loss of generality, we may ignore variable output wires. By construction of $B$, each output $b_i$ is of the form $\sum_{i=1}^n q_{ij}a_j+p_i$, with consistency equations obtained as above. We write this in terms of an $m\times n$ matrix $\mathsf{Q}$ and a column vector $\vec{p}$ of size $m$, namely
$$\vec{b} = \mathsf{Q} \vec{a}+ \vec{p}.$$
%
Let $r$ be the rank of $\mathsf{Q}$. We convert $\mathsf{Q}$ into reduced row echelon form $\mathsf{R}$, writing
$$\vec{b''}=\mathsf{R}\vec{a}.$$

By the properties of reduced row echelon form, after relabelling the $a_i$ and permuting the columns of $\mathsf{R}$ to match, we have that $b''_i=a_i + \sum_{j=l+1}^m r_{ij} a_j$ for $1\leq i\leq l$. Therefore, we may consider $a_{l+1},\ldots,a_m$ as free wires and express other $a_i$ as linear functions of these wires plus constants.

Note that if $b''_i\neq 0$ for some $i>l$, the circuit can never be satisfied anyway. However, assuming that our statement is a satisfiable circuit, with a witness consisting of satisfying wire values, this never occurs. Then the original circuit is satisfied if and only if the $a_i$ values satisfy the consistency equations.

If $\mathsf{Q}$ is an $m\times n$ matrix then it can be converted into reduced row echelon form using $O(\mathrm{max}(m,n) mn)$ operations. It is trivial that $m\leq 2\left| B \right|$ and $n\leq \left| B \right|$. This gives an upper bound of $O(\left| B \right|^3)$ computation for the output sub-circuit. 
%
Note that this is often a large over-estimate; this upper bound occurs for circuits of depth 1 where inputs feed into distinct gates. For circuits of large depth, where the same input is fed into several gates, the upper bound will definitely not be reached. %Recall that the circuit $B$ computes a linear map and can be written so that $\left| B \right| = O(mn)$. Then the computational cost is $O(\mathrm{max}(m,n) \left| B \right|)$.

The case of the input sub-circuit is very similar, except that we take the transpose of the matrix.

\begin{figure}
\centering
\scalebox{0.7}{
\begin{tikzpicture}[gate/.style={circle,draw,text width=0.2 cm, align=center}]
%level 1

\node[gate] (m1) {$\times$};
\node[above left= 0.1 cm and 0.8 cm of m1.160] (a1) {$a_1$};
\node[below left= 0.1 cm and 0.8 cm of m1.200] (b1) {$b_1$};
\draw[->] (a1) -- (m1.160);
\draw[->] (b1) -- (m1.200);


\node[gate, below = 0.6 cm of m1] (m2) {$\times$};

\node[ above left=0.1 cm and 0.8 cm of m2.160] (a2) {$a_2$};
\node[below left=0.1 cm and 0.8 cm of m2.200] (b2) {$b_2$};
\draw[->] (a2) -- (m2.160);
\draw[->] (b2) -- (m2.200);

\node[gate, below = of m2] (m3) {$\times$};

\node[above left=0.1 cm and 0.8 cm of m3.160,font=\bfseries] (a3) {$a_3$};
\node[below left=0.1 cm and 0.8 cm of m3.200,font=\bfseries] (b3) {$b_3$};
\draw[->] (a3) -- (m3.160);
\draw[->] (b3) -- (m3.200);




%level 2
\node[gate,below right= 0.15 cm and 1.4 cm of m1] (m4) {$\times$};

\node[gate,below right= 0.15 cm and 1.4 cm of m3] (cm1) {$\times$};
\node[ below right = 0.7 of m3] (cc) {$4$};


\draw[->] (m1) -- (m4) node [midway, above =0.1 cm  ] {$c_1=a_4$};
\draw[->] (m2) -- (m4) node [midway, below=0.1 cm] {$c_2=b_4$};

\draw[->] (m3) -- (cm1) node [midway, above] {$c_3$};
\draw[->] (cc) -- (cm1);


%level 3

\node[right = 0.7 of m4 ] (dot1) {$\bullet$ } ;
\node[gate, below = of dot1 ] (a1) {+};
\node[right = 0.7 of cm1] (dot2) {$\bullet$};

\draw[->] (dot1.center) -- (a1);
\draw[->] (dot2.center) -- (a1);


%level 4
\node[gate, right = 0.7 cm of dot1 ] (m5) {$\times$};
\node[ right =  0.7 cm of a1 ] (dot3) {$\bullet$};
\node[gate, right =0.7 cm  of dot2 ] (m6) {$\times$};
%\node[right = of m4 ] (dot1) {$\bullet$ } ;

%\node[right = of cm1 ] (dot2) {$\bullet$};


\draw[->] (m4) -- (m5) node [near start, above] {$c_4$} node [near end, above] {$a_5$};
\draw[->] (cm1) -- (m6) node [near end, below] {$b_6$};
\draw (a1) -- (dot3.center) ;


\draw[<->] (m5) -- (m6) node [near start, right ] {$b_5$} node [near end, right] {$a_6$};


%level 5

\node[ right = 0.7 cm of m5 ] (out1) {$c_5$};
\node[ right = 0.7 cm  of m6 ] (out2) {$c_6$};
\draw[->] (m5) -- (out1) ;
\draw[->] (m6) -- (out2) ;

%level 6
\node[ right =3 cm of m5] (eq) {$c_1=a_4$};
\node[below =0.1 cm of eq] (eq1) {$c_2=b_4$};
\node[below =0.1 cm of eq1] (eq2) {$c_4=a_5$};
\node[below =0.1 cm of eq2] (eq3) {$4c_3+c_4=b_5$};
\node[below =0.1 cm of eq3] (eq4) {$4c_3+c_4=a_6$};
\node[below =0.1 cm of eq4] (eq5) {$4c_3=b_6$};


\end{tikzpicture}
}
\vspace{-.2cm}
\caption{A simple arithmetic circuit, and the corresponding consistency equations. The first sub-circuit contains the wires $a_1,b_1,c_1,a_2,b_2,c_2,a_3,a_3,c_3$. The second sub-circuit contains the wires $c_1,a_4, c_2,b_4,c_4, c_5,c_6$. The third sub-circuit $B$ contains the wires $c_3,c_4,a_5,b_5,a_6$.}
\label{fig:digraph} 
\end{figure}

\subsection{Reduction of Circuit Satisfiability Problem to a Hadamard Matrix Product and Linear Constraints.}
Having preprocessed the arithmetic circuit as in the previous section, we may assume that the input and the output wires feed into and go out from multiplication gates only.
We number the multiplication gates from 1 to $N$ and we arrange the inputs and outputs of these gates into three $m\times n$ matrices $A,B$ and $C$ such that the $(i,j)$ entries of the matrices correspond to the left input, right input and output of the same multiplication gate.

As shown in \cite{BootleCCGP16}, an arithmetic circuit can be described as a system of equations in the entries of the above matrices. The multiplication gates define a set of $N$ equations 
\begin{equation}\label{eq1:product}
A \circ B = C
\end{equation}
where $\circ$ is the Hadamard (entry-wise) product. 
%
The circuit description also contains constraints on the wires between multiplication gates. %The output of one multiplication gate might feed into a combination of addition gates and multiplication by constant gates yielding one or more inputs to other multiplication gates. 
Denoting the rows of the matrices $A,B,C$ as
\begin{align*}\vec{a}_ {i}=(a_{i,1},\ldots,a_{i,n})&& \vec{b}_ {i}=(b_{i,1},\ldots,b_{i,n})&&\vec{c}_ {i}=(c_{i,1},\ldots,c_{i,n})&& \text{for }  i \in \{1,\ldots,m\}\end{align*}
these constraints can be expressed as $Q<2N$ linear equations of inputs and outputs of multiplication gates of the form
\begin{equation}\label{eq1:consistency}
\sum_{i=1}^m \vec{a}_{i} \cdot \vec{w}_{q,{a,i}}+\sum_{i=1}^m \vec{b}_{i} \cdot \vec{w}_{q,{b,i}}+\sum_{i=1}^m \vec{c}_{i} \cdot  \vec{w}_{q,{c,i}}=K_{q} \quad \text{ for } q \in\{1,\ldots,Q\}
\end{equation}
for constant vectors $\vec{w}_{q,{a,i}},\vec{w}_{q,{b,i}},\vec{w}_{q,{c,i}}$ and scalars $K_{q}$.

For example, suppose that the circuit contains a single addition gate, with $a_{1,1}$ and $a_{1,2}$ as inputs, and $b_{1,1}$ as output. In this case, $Q=1$ and we would set $\vec{w}_{1,a,1} = (1,1,0,\ldots,0)$, $\vec{w}_{1,b,1} = (-1,0,\ldots,0)$, and all other $\vec{w}$ vectors would be set to $\vec{0}$. Then~\eqref{eq1:consistency} would simply read
$$ a_{1,1} + a_{1,2} - b_{1,1} = 0$$
to capture the constraint imposed by the addition gate.

In total, to capture all multiplications and linear constraints, we have $N+Q$ equations that the wires must satisfy in order for the circuit to be satisfiable. %We can combine the equations into a polynomial of degree $N+Q$ in an indeterminate $Y$ such that the circuit is satisfiable if and only if the polynomial is identically 0.

\section{Commitment Schemes}
\label{sec:BComDef}
A non-interactive commitment scheme allows a sender to commit to a secret message and later reveal the message in a verifiable way. Here we are interested in commitment schemes that take as input an arbitrary length message so the message space is $\{0,1\}^*$. A commitment scheme is defined by a pair of PPT algorithms $(\ComSetup,\ComCommit)$.
\begin{description}
\item [$\ComSetup(1^\sep)\to ck$:]
Given a security parameter, this returns a commitment key $ck$.
\item [$\ComCommit_{ck}(m)\to \Co$:] Given a message $m$ from a message space $\mathcal{M}_{ck}$, this picks randomness $r\gets \mathcal{R}_{ck}$, from a randomness space, and computes a commitment $\Co=\ComCommit_{ck}(m;r) \in \mathcal{C}_{ck}$. 
\end{description}
A commitment scheme must be \emph{binding} and \emph{hiding}. The binding property means that it is infeasible to open a commitment to two different messages, whereas the hiding property means that the commitment does not reveal anything about the committed message. 
\begin{definition} [Binding]
A commitment scheme is \emph{computationally binding} if for all PPT adversaries $\A$
$$ \Pr\left[
\begin{array}{c}
ck \asn \ComSetup(1^\sep);~ (m_0,r_0,m_1,r_1)\asn \A(ck): \\
m_0\neq m_1 ~\wedge~ \ComCommit_{ck}(m_0;r_0)=\ComCommit_{ck}(m_1;r_1)
\end{array}
\right] \approx 0.
$$
If this holds also for unbounded adversaries, we say the commitment scheme is \emph{statistically binding}.
\end{definition}
\begin{definition} [Hiding]
A commitment scheme is \emph{computationally hiding} if for all PPT stateful adversaries $\A$
$$ \Pr\left[\begin{array}{c}ck \asn \ComSetup(1^\sep);~(m_0,m_1)\asn \A(ck);~b \gets \{0,1\};~ \\
\Co\gets \ComCommit_{ck}(m_b):~\A(\Co)=b
\end{array}\right] \approx \frac{1}{2},$$
where $\A$ outputs messages of equal length $|m_0|=|m_1|$.
If the definition holds also for unbounded adversaries, we say the commitment scheme is \emph{statistically hiding}.
\end{definition}

Suppose further that $(\mathcal{M}_{ck},\cdot)$, $(\mathcal{R}_{ck},\circ)$ and $(\mathcal{C}_{ck},\oplus)$ are groups.
\begin{definition}[Homomorphic Commitment Scheme]
We call the commitment scheme homomorphic if $\ComCommit:\mathcal{M}_{ck}\times\mathcal{R}_{ck}\to \mathcal{C}_{ck}$ is a group-homomorphism, i.e. 
$$\ComCommit(m \cdot m';r\circ r')=\ComCommit(m;r)\oplus \ComCommit(m';r')$$
\end{definition}

\section{Error-Correcting Codes}\label{sec:LC}
A \emph{code} over an alphabet $\Sigma$ is a subset $\codeset\subseteq \Sigma^n$. A code $\codeset$ is associated with an encoding function $E_\codeset:\Sigma^k\to \Sigma^n$ mapping messages of length $k$ into \emph{codewords} of length $n$.
We assume there is a setup algorithm ${\text{Gen}}_{\preECC}$ which takes as input a finite field $\F$ and the parameter $k \in \N$, and outputs an encoding function $E_\codeset$.

We use error-correcting codes as part of our compilation from \ILC\ protocols to proof systems based on collision-resistant hash-functions. We restrict our attention to $\F$-\emph{linear codes} for which the alphabet is a finite field $\F$, the code $\codeset$ is a $k$-dimensional linear subspace of $\F^n$, and $E_\codeset$ is an $\F$-linear map. The \emph{rate} of the code is defined to be $\frac{k}{n}$. The \emph{Hamming distance} between two vectors $\vec{x},\vec{y} \in \F^n$ is denoted by $\hamdist(\vec{x},\vec{y})$ and corresponds to the number of coordinates in which $\vec{x},\vec{y}$ differ. The \emph{(minimum) distance} of a code is defined to be the minimum Hamming distance $\minhamdist$ between distinct codewords in $\codeset$. %, and the \emph{relative distance} $\relmindist=\frac{\minhamdist}{n}$. 
We denote by $[n,k,\minhamdist]_\F$ a linear code over $\F$ with length $n$, dimension $k$ and minimum distance $\minhamdist$. The \emph{Hamming weight} of a vector $\vec{x}$ is $\hamweight(\vec{x})=|\{i \in [n]: \vec{x}_i \neq 0\}|$.

%The Hamming distance satisfies the triangle inequality, $\hamdist(\vec{a},\vec{c})\leq \hamdist(\vec{a},\vec{b})+\hamdist(\vec{b},\vec{c})$ as well as $\hamdist(\vec{a}+\vec{c},\vec{b}+\vec{d})\leq \hamdist(\vec{a},\vec{b})+\hamdist(\vec{c},\vec{d})$. 

To get good results when compiling , we will use families of linear codes achieving asymptotically good parameters. More precisely, we require codes with linear length, $n=\Theta(k)$, and linear distance, $\minhamdist=\Theta(k)$, in the \emph{dimension} $k$ of the code.
We recall that random linear codes achieve with high probability the best trade-off between distance and rate. However, in this work we are concerned with the computational efficiency of the encoding procedure and random codes are not known to be very efficient. The reader can keep Reed-Solomon codes in mind for concrete instantiations, as these as simple, satisfy all of the necessary requirements, and are practically efficient as well as asymptotically.